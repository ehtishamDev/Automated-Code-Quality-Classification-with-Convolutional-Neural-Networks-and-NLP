{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nXO2g57lXrZH",
        "outputId": "1581e6c2-57ad-402d-8806-dd47d856135c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.layers import Dense, Input, GlobalMaxPooling1D\n",
        "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
        "from keras.models import Model\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "3c-iq-QlgL-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define the path of the dataset\n",
        "dataset_path = 'Dataset'"
      ],
      "metadata": {
        "id": "1cpqGRdogfgF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create lists for storing the tokens\n",
        "good_tokens = []\n",
        "bad_tokens = []"
      ],
      "metadata": {
        "id": "dnqNiUk1gqzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Loop through the folders in the dataset\n",
        "for root, folders, files in os.walk(dataset_path):\n",
        "    for file in files:\n",
        "        #Check if it is a C++ file\n",
        "        if file.endswith(\".c\"):\n",
        "            #Open the file\n",
        "            with open(os.path.join(root, file), 'r') as f:\n",
        "                current_file = f.read()\n",
        "                #Tokenize the file content\n",
        "                current_tokens = word_tokenize(current_file)\n",
        "                #Remove stopwords\n",
        "                current_words = [word for word in current_tokens if word not in stopwords.words('english')]\n",
        "                #Remove punctuation\n",
        "                current_words = [word for word in current_words if word not in string.punctuation]\n",
        "                #Lemmatize the words\n",
        "                current_words = [WordNetLemmatizer().lemmatize(word) for word in current_words]\n",
        "                #Check if it is a good function\n",
        "                if 'good' in root:\n",
        "                    good_tokens.extend(current_words)\n",
        "                else:\n",
        "                    bad_tokens.extend(current_words)"
      ],
      "metadata": {
        "id": "vCZZYN4fgq2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Concatenate good and bad tokens and create labels\n",
        "all_tokens = good_tokens + bad_tokens\n",
        "labels = [0]*len(good_tokens) + [1]*len(bad_tokens)"
      ],
      "metadata": {
        "id": "3KJUE-PTgwA2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Split the dataset into train, test and validation\n",
        "X_train, X_test, y_train, y_test = train_test_split(all_tokens, labels, test_size=0.2, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)"
      ],
      "metadata": {
        "id": "DBdUjC7xgq5Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a tokenizer object\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(X_train)"
      ],
      "metadata": {
        "id": "otlB9eFZgq8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert data into sequences\n",
        "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
        "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
        "X_val_seq = tokenizer.texts_to_sequences(X_val)"
      ],
      "metadata": {
        "id": "XszD_NJ7hAFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Pad the sequences\n",
        "max_length = max([len(x) for x in all_tokens])\n",
        "X_train_pad = pad_sequences(X_train_seq, maxlen=max_length, padding='post')\n",
        "X_test_pad = pad_sequences(X_test_seq, maxlen=max_length, padding='post')\n",
        "X_val_pad = pad_sequences(X_val_seq, maxlen=max_length, padding='post')"
      ],
      "metadata": {
        "id": "vGQ83VzshAMH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create the model\n",
        "embedding_dims = 50\n",
        "\n",
        "inputs = Input(shape=(max_length,))\n",
        "x = Embedding(len(tokenizer.word_index) + 1, embedding_dims)(inputs)\n",
        "x = Conv1D(128, 3, activation='relu')(x)\n",
        "x = MaxPooling1D(3)(x)\n",
        "x = Conv1D(128, 3, activation='relu')(x)\n",
        "x = MaxPooling1D(3)(x)\n",
        "x = Conv1D(128, 3, activation='relu')(x)\n",
        "x = GlobalMaxPooling1D()(x)\n",
        "x = Dense(128, activation='relu')(x)\n",
        "predictions = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=predictions)\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])"
      ],
      "metadata": {
        "id": "wYUzNC4ehAOl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Train the model\n",
        "model.fit(X_train_pad, np.array(y_train), batch_size=32, epochs=10, validation_data=(X_val_pad, np.array(y_val)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZ2JXJIihSyT",
        "outputId": "af65df96-73c9-40dc-e2f6-8abc49141c55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "4414/4414 [==============================] - 89s 20ms/step - loss: 4.8968e-04 - acc: 1.0000 - val_loss: 2.6560e-08 - val_acc: 1.0000\n",
            "Epoch 2/10\n",
            "4414/4414 [==============================] - 88s 20ms/step - loss: 2.6563e-08 - acc: 1.0000 - val_loss: 2.6560e-08 - val_acc: 1.0000\n",
            "Epoch 3/10\n",
            "4414/4414 [==============================] - 89s 20ms/step - loss: 2.6563e-08 - acc: 1.0000 - val_loss: 2.6560e-08 - val_acc: 1.0000\n",
            "Epoch 4/10\n",
            "4414/4414 [==============================] - 89s 20ms/step - loss: 2.6563e-08 - acc: 1.0000 - val_loss: 2.6560e-08 - val_acc: 1.0000\n",
            "Epoch 5/10\n",
            "4414/4414 [==============================] - 88s 20ms/step - loss: 2.6563e-08 - acc: 1.0000 - val_loss: 2.6560e-08 - val_acc: 1.0000\n",
            "Epoch 6/10\n",
            "4414/4414 [==============================] - 89s 20ms/step - loss: 2.6563e-08 - acc: 1.0000 - val_loss: 2.6560e-08 - val_acc: 1.0000\n",
            "Epoch 7/10\n",
            "4414/4414 [==============================] - 87s 20ms/step - loss: 2.6563e-08 - acc: 1.0000 - val_loss: 2.6560e-08 - val_acc: 1.0000\n",
            "Epoch 8/10\n",
            "4414/4414 [==============================] - 89s 20ms/step - loss: 2.6563e-08 - acc: 1.0000 - val_loss: 2.6560e-08 - val_acc: 1.0000\n",
            "Epoch 9/10\n",
            "4414/4414 [==============================] - 87s 20ms/step - loss: 2.6563e-08 - acc: 1.0000 - val_loss: 2.6560e-08 - val_acc: 1.0000\n",
            "Epoch 10/10\n",
            "4414/4414 [==============================] - 91s 21ms/step - loss: 2.6563e-08 - acc: 1.0000 - val_loss: 2.6560e-08 - val_acc: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5e4dfaaf40>"
            ]
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluate the model\n",
        "y_test = np.array(y_test)\n",
        "loss, acc = model.evaluate(X_test_pad, y_test, verbose=1)\n",
        "print(\"Accuracy:\", acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8pifCWOhASA",
        "outputId": "c2671a0a-1a5c-4bb6-f924-39639f7a3942"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1472/1472 [==============================] - 9s 6ms/step - loss: 2.6559e-08 - acc: 1.0000\n",
            "Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YG2MueU0jrIF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}